<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shailendra Jain - Personal Webpage</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }

        header {
            background-color: #333;
            color: white;
            padding: 1rem 0;
            text-align: center;
        }

        .container {
            width: 80%;
            margin: auto;
            overflow: hidden;
        }

        .bio,
        .contact,
        .interests,
        .projects {
            background: white;
            margin: 1rem 0;
            padding: 1rem;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }

        h2 {
            color: #333;
        }

        a {
            color: #333;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }
    </style>
</head>

<body>
    <header>
        <h1>Shailendra Jain</h1>
        <p>SWE-III Machine Learning Engineer at JP Morgan Chase, London</p>
    </header>

    <div class="container">
        <section class="bio">
            <h2>Bio</h2>
            <p>Results-driven Machine Learning Operations Engineer with over 11 years of experience in deploying
                scalable and reliable machine learning models into production. Expert in leveraging Amazon SageMaker,
                FAST API, and EKS for containerized deployments, and proficient in designing complex data pipelines and
                real-time inference systems. Demonstrated ability to collaborate with data scientists and lead
                end-to-end solution architecture, resulting in a 30% reduction in latency and a 40% improvement in ETL
                processing times. Proven track record in enhancing system reliability to achieve a 99.9% uptime and
                increasing productivity by 25% through automation with Airflow and cron jobs. Adept at optimizing CI/CD
                pipelines using Bitbucket and Jenkins, accelerating deployment times by 50% and ensuring seamless
                integration and continuous improvement.</p>
        </section>

        <section class="contact">
            <h2>Contact Information</h2>
            <p>Email: <a href="mailto:shailendrajain28@gmail.com">shailendrajain28@gmail.com</a></p>
            <p>LinkedIn: <a href="https://linkedin.com/in/shailendrajain28"
                    target="_blank">linkedin.com/in/shailendrajain28</a></p>
            <p>GitHub: <a href="https://github.com/shailendrajain2892" target="_blank">github.com/shailendrajain2892</a>
            </p>
        </section>

        <section class="interests">
            <h2>Interests and Hobbies</h2>
            <p>Curious about new evolving tech and Machine Learning.</p>
            <p>Hobbies: Cricket, outdoor activities.</p>
        </section>

        <section class="projects">
            <h2>Project: End-to-End Machine Learning Model Deployment and Data Pipeline Automation</h2>
            <ul>
                <li><strong>Collaborated with Data Scientists to Deploy ML Models:</strong> Partnered with data
                    scientists to ideate and implement solutions for deploying machine learning models into production
                    using Amazon SageMaker and FAST API on EKS (Elastic Kubernetes Service), leveraging containerization
                    for efficient and scalable deployments.</li>
                <li><strong>Implemented Real-Time Inference Pipelines:</strong> Successfully hosted machine learning
                    models on GPU machines, enabling real-time inference pipelines that handle large volumes of data.
                    Employed auto-scaling groups to dynamically adjust resources based on CPU utilization, ensuring
                    robust performance and availability across multiple availability zones to mitigate the risk of
                    failure.</li>
                <li><strong>Engineered Complex Data Pipelines:</strong> Developed and maintained data pipelines to
                    extract, transform, and load (ETL) large datasets from various lines of business (LOBs) into both
                    on-premises and cloud-based S3 buckets. This involved setting up and configuring complex
                    environments and using tools like CADS (Custom Application Data Sources).</li>
                <li><strong>Automated Workflow Scheduling:</strong> Scheduled and managed data pipeline workflows using
                    Apache Airflow and cron jobs, ensuring timely and periodic data processing. This automation
                    facilitated seamless data integration and consistency across the organization.</li>
                <li><strong>Streamlined CI/CD Processes:</strong> Utilized Bitbucket for version control and Jenkins for
                    continuous integration/continuous deployment (CI/CD) pipelines, ensuring efficient code deployment
                    and minimizing downtime.</li>
                <li><strong>Architected End-to-End Solutions:</strong> Worked closely with data science leads to
                    understand the specific requirements of machine learning models, architecting comprehensive
                    solutions and taking full ownership of deploying these models from ideation to production.</li>
            </ul>
            <h2>Achievements</h2>
            <ul>
                <li><strong>Optimized ML Model Deployment:</strong> Spearheaded the deployment of machine learning
                    models that improved the efficiency of real-time data processing, reducing latency by 30% through
                    optimized GPU utilization and auto-scaling mechanisms.</li>
                <li><strong>Enhanced System Reliability:</strong> Implemented multi-zone hosting and auto-scaling
                    features that increased system reliability, achieving a 99.9% uptime and significantly reducing the
                    risk of failures during peak usage times.</li>
                <li><strong>Efficient Data Pipeline Management:</strong> Developed robust data pipelines that
                    streamlined data processing tasks, reducing ETL processing times by 40% and enhancing data
                    availability for machine learning applications.</li>
                <li><strong>Automated and Scalable Solutions:</strong> Introduced automation in workflow scheduling with
                    Airflow and cron jobs, which improved data processing efficiency and consistency, leading to a 25%
                    increase in overall productivity.</li>
                <li><strong>Leadership in Solution Architecture:</strong> Led the end-to-end deployment of machine
                    learning models, ensuring seamless integration with existing systems and enhancing collaboration
                    with data science teams, resulting in a more agile and responsive development environment.</li>
                <li><strong>CI/CD Pipeline Optimization:</strong> Enhanced the CI/CD pipeline using Bitbucket and
                    Jenkins, which accelerated deployment times by 50% and reduced errors, contributing to a more
                    reliable and efficient development process.</li>
            </ul>
        </section>
    </div>
</body>

</html>